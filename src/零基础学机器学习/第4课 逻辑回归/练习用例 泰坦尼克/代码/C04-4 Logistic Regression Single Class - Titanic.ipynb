{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd # 导入pandas库\nimport numpy as np  # 导入numpy库\ndf_titanic = pd.read_csv('../input/titanic/train.csv') # 读取文件\ndf_titanic.head() # 显示前5行数据","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_titanic.Survived.value_counts() # 输出分类值，及各个类别数目\ndf_titanic['Age'] = df_titanic['Age'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 把类别型变量转换为哑变量\na = pd.get_dummies(df_titanic['Sex'], prefix = \"Sex\")\nb = pd.get_dummies(df_titanic['Embarked'], prefix = \"Em\")\n# 把哑变量添加进dataframe\nframes = [df_titanic, a, b]\ndf_titanic = pd.concat(frames, axis = 1)\ndf_titanic = df_titanic.drop(columns = ['Sex', 'Embarked'])\ndf_titanic.head() # 显示新的dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_titanic.drop(['Survived','Name','Ticket','Cabin'], axis=1) # 拿掉比较不相关的字段,构建特征集\ny = df_titanic.Survived.values # 构建标签集\ny = y.reshape(-1,1) # -1是相对索引，等价于len(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler # 导入数据缩放器\nscaler = StandardScaler() # 选择归一化数据缩放器，MinMaxScaler\nX_train = scaler.fit_transform(X_train) # 特征归一化 训练集fit_transform\nX_test = scaler.transform(X_test) # 特征归一化 测试集transform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression #导入逻辑回归模型\nlr = LogisticRegression() # lr,就代表是逻辑回归模型\nlr.fit(X_train,y_train) # fit,就相当于是梯度下降\nprint(\"SK-learn逻辑回归测试准确率 {:.2f}%\".format(lr.score(X_test,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 首先定义一个sigmoid函数，输入Z，返回y'\ndef sigmoid(z):    \n    y_hat = 1/(1+ np.exp(-z))\n    return y_hat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 然后定义损失函数\ndef cost_function(X,y,w,b):\n    y_hat = sigmoid(np.dot(X,w) + b) # Sigmoid逻辑函数 + 线性函数（wX+b）得到y'\n#     print (X.shape,w.shape)\n    loss = np.abs((y*np.log(y_hat) + (1-y)*np.log(1-y_hat))) # 计算损失\n#     cost = np.sum(loss) / X.shape[0]  # 返回整个数据集平均损失  \n    cost = np.mean(loss) # 返回整个数据集平均损失\n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(X,y,w,b,lr,iter) : #定义逻辑回归梯度下降函数\n    l_history = np.zeros(iter) # 初始化记录梯度下降过程中误差值(损失)的数组\n    w_history = np.zeros((iter,w.shape[0],w.shape[1])) # 初始化权重记录的数组\n    b_history = np.zeros(iter) # 初始化记录梯度下降过程中偏置的数组  \n    for i in range(iter): #进行机器训练的迭代\n        y_hat = sigmoid(np.dot(X,w) + b) #Sigmoid逻辑函数+线性函数(wX+b)得到y'\n        loss = (y*np.log(y_hat) + (1-y)*np.log(1-y_hat))\n        derivative_w = np.dot(X.T,((y_hat-y)))/X.shape[0]  # 给权重向量求导\n        derivative_b = np.sum(y_hat-y)/X.shape[0] # 给偏置求导\n        w = w - lr * derivative_w # 更新权重向量，lr即学习速率alpha\n        b = b - lr * derivative_b   # 更新偏置，lr即学习速率alpha\n        l_history[i] =  cost_function(X,y,w,b) # 梯度下降过程中的损失\n        print (\"轮次\", i+1 , \"当前轮训练集损失：\",l_history[i])        \n        w_history[i] = w # 梯度下降过程中权重的历史 请注意w_history和w的形状\n        b_history[i] = b # 梯度下降过程中偏置的历史\n    return l_history, w_history, b_history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(X,w,b): # 定义预测函数\n    z = np.dot(X,w) + b # 线性函数\n    y_hat = sigmoid(z) # 逻辑函数转换\n    y_pred = np.zeros((y_hat.shape[0],1)) # 初始化预测结果变量    \n    for i in range(y_hat.shape[0]):\n        if y_hat[i,0] < 0.5:\n            y_pred[i,0] = 0 # 如果预测概率小于0.5，输出分类0\n        else:\n            y_pred[i,0] = 1 # 如果预测概率大于0.5，输出分类0\n    return y_pred # 返回预测分类的结果","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(X,y,w,b,lr,iter): # 定义逻辑回归模型\n    l_history,w_history,b_history = gradient_descent(X,y,w,b,lr,iter)#梯度下降\n    print(\"训练最终损失:\", l_history[-1]) # 打印最终损失\n    y_pred = predict(X,w_history[-1],b_history[-1]) # 进行预测\n    traning_acc = 100 - np.mean(np.abs(y_pred - y_train))*100 # 计算准确率\n    print(\"逻辑回归训练准确率: {:.2f}%\".format(traning_acc))  # 打印准确率\n    return l_history, w_history, b_history # 返回训练历史记录","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#初始化参数\ndimension = X.shape[1] # 这里的维度 len(X)是矩阵的行的数，维度是列的数目\nweight = np.full((dimension,1),0.1) # 权重向量，向量一般是1D，但这里实际上创建了2D张量\nbias = 0 # 偏置值\n#初始化超参数\nalpha = 1 # 学习速率\niterations = 100 # 迭代次数","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 用逻辑回归函数训练机器\nloss_history, weight_history, bias_history = logistic_regression(X_train,y_train,\n                                                                 weight,bias,                                                                 \n                                                                 alpha,iterations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = predict(X_test,weight_history[-1],bias_history[-1]) # 预测测试集\ntesting_acc = 100 - np.mean(np.abs(y_pred - y_test))*100 # 计算准确率\nprint(\"逻辑回归测试准确率: {:.2f}%\".format(testing_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt # 导入绘图工具\nloss_history_test = np.zeros(iterations) # 初始化历史损失\nfor i in range(iterations): #求训练过程中不同参数带来的测试集损失\n    loss_history_test[i] = cost_function(X_test,y_test,weight_history[i],bias_history[i])\nindex = np.arange(0,iterations,1)\nplt.plot(index,loss_history,c='blue',linestyle='solid')\nplt.plot(index,loss_history_test,c='red',linestyle='dashed')\nplt.legend([\"Training Loss\", \"Test Loss\"])\nplt.xlabel(\"Number of Iteration\")\nplt.ylabel(\"Cost\")\nplt.show() # 同时显示显示训练集和测试集损失曲线","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}