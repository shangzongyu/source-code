{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # 导入NumPy数学工具箱\nimport pandas as pd # 导入Pandas数据处理工具箱\ndf_heart = pd.read_csv(\"../input/heart-dataset/heart.csv\")  # 读取文件\ndf_heart.head() # 显示前5行数据","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:05.554596Z","iopub.execute_input":"2021-12-09T09:11:05.554965Z","iopub.status.idle":"2021-12-09T09:11:05.618934Z","shell.execute_reply.started":"2021-12-09T09:11:05.554873Z","shell.execute_reply":"2021-12-09T09:11:05.618251Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df_heart.target.value_counts() # 输出分类值，及各个类别数目","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:05.620799Z","iopub.execute_input":"2021-12-09T09:11:05.621739Z","iopub.status.idle":"2021-12-09T09:11:05.636328Z","shell.execute_reply.started":"2021-12-09T09:11:05.621685Z","shell.execute_reply":"2021-12-09T09:11:05.635391Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt # 导入绘图工具\n# 以年龄+最大心率作为输入，查看分类结果散点图\nplt.scatter(x=df_heart.age[df_heart.target==1],\n            y=df_heart.thalach[(df_heart.target==1)], c=\"red\")\nplt.scatter(x=df_heart.age[df_heart.target==0],\n            y=df_heart.thalach[(df_heart.target==0)], marker='^')\nplt.legend([\"Disease\", \"No Disease\"]) # 显示图例\nplt.xlabel(\"Age\") # X轴-Age\nplt.ylabel(\"Heart Rate\") # Y轴-Heart Rate\nplt.show() # 显示散点图","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:05.638478Z","iopub.execute_input":"2021-12-09T09:11:05.639167Z","iopub.status.idle":"2021-12-09T09:11:05.932135Z","shell.execute_reply.started":"2021-12-09T09:11:05.639113Z","shell.execute_reply":"2021-12-09T09:11:05.930933Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 把3个文本型变量转换为哑变量\na = pd.get_dummies(df_heart['cp'], prefix = \"cp\")\nb = pd.get_dummies(df_heart['thal'], prefix = \"thal\")\nc = pd.get_dummies(df_heart['slope'], prefix = \"slope\")\n# 把哑变量添加进dataframe\nframes = [df_heart, a, b, c]\ndf_heart = pd.concat(frames, axis = 1)\ndf_heart = df_heart.drop(columns = ['cp', 'thal', 'slope'])\ndf_heart.head() # 显示新的dataframe","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:05.934432Z","iopub.execute_input":"2021-12-09T09:11:05.934733Z","iopub.status.idle":"2021-12-09T09:11:06.095005Z","shell.execute_reply.started":"2021-12-09T09:11:05.934698Z","shell.execute_reply":"2021-12-09T09:11:06.093987Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X = df_heart.drop(['target'], axis = 1) # 构建特征集\ny = df_heart.target.values # 构建标签集\ny = y.reshape(-1,1) # -1是\t相对索引，等价于len(y)\nprint(\"张量X的形状:\", X.shape)\nprint(\"张量X的形状:\", y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:06.096360Z","iopub.execute_input":"2021-12-09T09:11:06.097343Z","iopub.status.idle":"2021-12-09T09:11:06.107551Z","shell.execute_reply.started":"2021-12-09T09:11:06.097289Z","shell.execute_reply":"2021-12-09T09:11:06.106362Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:06.108799Z","iopub.execute_input":"2021-12-09T09:11:06.109156Z","iopub.status.idle":"2021-12-09T09:11:07.168409Z","shell.execute_reply.started":"2021-12-09T09:11:06.109110Z","shell.execute_reply":"2021-12-09T09:11:07.167501Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler # 导入数据缩放器\nscaler = MinMaxScaler() # 选择归一化数据缩放器，MinMaxScaler\nX_train = scaler.fit_transform(X_train) # 特征归一化 训练集fit_transform\nX_test = scaler.transform(X_test) # 特征归一化 测试集transform","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:07.169523Z","iopub.execute_input":"2021-12-09T09:11:07.169745Z","iopub.status.idle":"2021-12-09T09:11:07.182016Z","shell.execute_reply.started":"2021-12-09T09:11:07.169719Z","shell.execute_reply":"2021-12-09T09:11:07.181316Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# 首先定义一个Sigmoid函数，输入Z，返回y'\ndef sigmoid(z):    \n    y_hat = 1/(1+ np.exp(-z))\n    return y_hat    ","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:07.183104Z","iopub.execute_input":"2021-12-09T09:11:07.183348Z","iopub.status.idle":"2021-12-09T09:11:07.193595Z","shell.execute_reply.started":"2021-12-09T09:11:07.183319Z","shell.execute_reply":"2021-12-09T09:11:07.192463Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# 然后定义损失函数\ndef loss_function(X,y,w,b):\n    y_hat = sigmoid(np.dot(X,w) + b) # Sigmoid逻辑函数 + 线性函数(wX+b)得到y'\n    loss = -(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) # 计算损失\n    cost = np.sum(loss) / X.shape[0]  # 整个数据集平均损失    \n    return cost # 返回整个数据集平均损失","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:07.194841Z","iopub.execute_input":"2021-12-09T09:11:07.195094Z","iopub.status.idle":"2021-12-09T09:11:07.215163Z","shell.execute_reply.started":"2021-12-09T09:11:07.195040Z","shell.execute_reply":"2021-12-09T09:11:07.214161Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# 然后构建梯度下降的函数\ndef gradient_descent(X,y,w,b,lr,iter) : #定义逻辑回归梯度下降函数\n    l_history = np.zeros(iter) # 初始化记录梯度下降过程中误差值(损失)的数组\n    w_history = np.zeros((iter,w.shape[0],w.shape[1])) # 初始化权重记录的数组\n    b_history = np.zeros(iter) # 初始化记录梯度下降过程中偏置的数组  \n    for i in range(iter): #进行机器训练的迭代\n        y_hat = sigmoid(np.dot(X,w) + b) #Sigmoid逻辑函数+线性函数(wX+b)得到y'\n        derivative_w = np.dot(X.T,((y_hat-y)))/X.shape[0]  # 给权重向量求导\n        derivative_b = np.sum(y_hat-y)/X.shape[0] # 给偏置求导\n        w = w - lr * derivative_w # 更新权重向量，lr即学习速率alpha\n        b = b - lr * derivative_b   # 更新偏置，lr即学习速率alpha\n        l_history[i] =  loss_function(X,y,w,b) # 梯度下降过程中的损失\n        print (\"轮次\", i+1 , \"当前轮训练集损失：\",l_history[i]) \n        w_history[i] = w # 梯度下降过程中权重的历史 请注意w_history和w的形状\n        b_history[i] = b # 梯度下降过程中偏置的历史\n    return l_history, w_history, b_history\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:07.218310Z","iopub.execute_input":"2021-12-09T09:11:07.218614Z","iopub.status.idle":"2021-12-09T09:11:07.237742Z","shell.execute_reply.started":"2021-12-09T09:11:07.218539Z","shell.execute_reply":"2021-12-09T09:11:07.236850Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def predict(X,w,b): # 定义预测函数\n    z = np.dot(X,w) + b # 线性函数\n    y_hat = sigmoid(z) # 逻辑函数转换\n    y_pred = np.zeros((y_hat.shape[0],1)) # 初始化预测结果变量  \n    for i in range(y_hat.shape[0]):\n        if y_hat[i,0] < 0.5:\n            y_pred[i,0] = 0 # 如果预测概率小于0.5，输出分类0\n        else:\n            y_pred[i,0] = 1 # 如果预测概率大于0.5，输出分类1\n    return y_pred # 返回预测分类的结果","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:07.238811Z","iopub.execute_input":"2021-12-09T09:11:07.239085Z","iopub.status.idle":"2021-12-09T09:11:07.252551Z","shell.execute_reply.started":"2021-12-09T09:11:07.239033Z","shell.execute_reply":"2021-12-09T09:11:07.251654Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def logistic_regression(X,y,w,b,lr,iter): # 定义逻辑回归模型\n    l_history,w_history,b_history = gradient_descent(X,y,w,b,lr,iter)#梯度下降\n    print(\"训练最终损失:\", l_history[-1]) # 打印最终损失\n    y_pred = predict(X,w_history[-1],b_history[-1]) # 进行预测\n    traning_acc = 100 - np.mean(np.abs(y_pred - y_train))*100 # 计算准确率\n    print(\"逻辑回归训练准确率: {:.2f}%\".format(traning_acc))  # 打印准确率\n    return l_history, w_history, b_history # 返回训练历史记录","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:07.253656Z","iopub.execute_input":"2021-12-09T09:11:07.253943Z","iopub.status.idle":"2021-12-09T09:11:07.268165Z","shell.execute_reply.started":"2021-12-09T09:11:07.253911Z","shell.execute_reply":"2021-12-09T09:11:07.267227Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#初始化参数\ndimension = X.shape[1] # 这里的维度 len(X)是矩阵的行的数，维度是列的数目\nweight = np.full((dimension,1),0.1) # 权重向量，向量一般是1D，但这里实际上创建了2D张量\nbias = 0 # 偏置值\n#初始化超参数\nalpha = 1 # 学习速率\niterations = 500 # 迭代次数","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:07.269307Z","iopub.execute_input":"2021-12-09T09:11:07.269560Z","iopub.status.idle":"2021-12-09T09:11:07.287884Z","shell.execute_reply.started":"2021-12-09T09:11:07.269528Z","shell.execute_reply":"2021-12-09T09:11:07.286845Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# 用逻辑回归函数训练机器\nloss_history, weight_history, bias_history =  \\\n            logistic_regression(X_train,y_train,weight,bias,alpha,iterations)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:07.289481Z","iopub.execute_input":"2021-12-09T09:11:07.289971Z","iopub.status.idle":"2021-12-09T09:11:07.763746Z","shell.execute_reply.started":"2021-12-09T09:11:07.289923Z","shell.execute_reply":"2021-12-09T09:11:07.762692Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"y_pred = predict(X_test,weight_history[-1],bias_history[-1]) # 预测测试集\ntesting_acc = 100 - np.mean(np.abs(y_pred - y_test))*100 # 计算准确率\nprint(\"逻辑回归测试准确率: {:.2f}%\".format(testing_acc))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:07.765423Z","iopub.execute_input":"2021-12-09T09:11:07.766218Z","iopub.status.idle":"2021-12-09T09:11:07.774176Z","shell.execute_reply.started":"2021-12-09T09:11:07.766167Z","shell.execute_reply":"2021-12-09T09:11:07.773083Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print (\"逻辑回归预测分类值:\",predict(X_test,weight_history[-1],bias_history[-1]))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:07.775647Z","iopub.execute_input":"2021-12-09T09:11:07.776005Z","iopub.status.idle":"2021-12-09T09:11:07.789106Z","shell.execute_reply.started":"2021-12-09T09:11:07.775964Z","shell.execute_reply":"2021-12-09T09:11:07.788257Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"loss_history_test = np.zeros(iterations) # 初始化历史损失\nfor i in range(iterations): #求训练过程中不同参数带来的测试集损失\n    loss_history_test[i] = loss_function(X_test,y_test,\n                                         weight_history[i],bias_history[i])\nindex = np.arange(0,iterations,1)\nplt.plot(index, loss_history,c='blue',linestyle='solid')\nplt.plot(index, loss_history_test,c='red',linestyle='dashed')\nplt.legend([\"Training Loss\", \"Test Loss\"])\nplt.xlabel(\"Number of Iteration\")\nplt.ylabel(\"Cost\")\nplt.show() # 同时显示显示训练集和测试集损失曲线","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:07.790169Z","iopub.execute_input":"2021-12-09T09:11:07.790463Z","iopub.status.idle":"2021-12-09T09:11:08.064036Z","shell.execute_reply.started":"2021-12-09T09:11:07.790422Z","shell.execute_reply":"2021-12-09T09:11:08.063003Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression #导入逻辑回归模型\nlr = LogisticRegression() # lr,就代表是逻辑回归模型\nlr.fit(X_train,y_train) # fit,就相当于是梯度下降\nprint(\"SK-learn逻辑回归测试准确率{:.2f}%\".format(lr.score(X_test,y_test)*100))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:11:08.066095Z","iopub.execute_input":"2021-12-09T09:11:08.066354Z","iopub.status.idle":"2021-12-09T09:11:08.178904Z","shell.execute_reply.started":"2021-12-09T09:11:08.066324Z","shell.execute_reply":"2021-12-09T09:11:08.177975Z"},"trusted":true},"execution_count":18,"outputs":[]}]}