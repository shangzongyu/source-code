{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # 导入NumPy数学工具箱\nimport pandas as pd # 导入Pandas数据处理工具箱\nfrom keras.datasets import boston_housing #从Keras中导入mnist数据集\n#读入训练集和测试集\n(X_train, y_train), (X_test, y_test) = boston_housing.load_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cost_function(X, y, W): # 手工定义一个MSE均方误差函数，W此时是一个向量\n# X -> 是一个矩阵，形状是(N,4),N是数据集大小，4是特征数量\n# W -> 是一个向量，形状是(4,1)（1*）\n#   y_hat = X.dot(weight) # 这是假设函数,其中已经应用了Python的广播功能\n#   y_hat = np.dot(X,weight) # 也是正确的\n    y_hat = X.dot(W.T) # 也是正确的 点积运算 h(x)=w_0*x_0 + w_1*x_1 + w_2*x_2 + w_3*x_3    \n#   y_hat = np.dot(X,weight.T) # 也是正确的\n#   y_hat = weight.dot(X) # 错误 shapes (4,) and (160,4) not aligned: 4 (dim 0) != 160 (dim 0)\n#   y_hat = np.dot(weight,X) # 错误 shapes (4,) and (160,4) not aligned: 4 (dim 0) != 160 (dim 0)\n    loss = y_hat-y # 求出每一个y’和训练集中真实的y之间的差异 \n    cost = np.sum(loss**2)/len(X) # 这是均方误差函数的代码实现\n    return cost # 返回当前模型的均方误差值","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(X, y, W, lr, iter): # 定义梯度下降函数\n    l_history = np.zeros(iter) # 初始化记录梯度下降过程中损失的数组\n    W_history = np.zeros((iter,len(W))) # 初始化权重数组 \n    for iter in range(iter): # 进行梯度下降的迭代，就是下多少级台阶\n        y_hat = X.dot(W) # 这个是向量化运行实现的假设函数   \n        loss = y_hat-y # 中间过程, y_hat和y真值的差\n        derivative_W = X.T.dot(loss)/(2*len(X)) #求出多项式的梯度向量\n        derivative_W = derivative_W.reshape(len(W)) \n        W = W - alpha*derivative_W # 结合下降速率更新权重\n        l_history[iter] = cost_function(X, y, W) # 损失的历史记录 \n        W_history[iter] = W # 梯度下降过程中权重的历史记录\n    return l_history, W_history # 返回梯度下降过程数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#首先确定参数的初始值\niterations = 12000; # 迭代12000次\nalpha = 0.00001; #学习速率设为0.00001\nweight = np.array([0.5,1.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]) # 权重向量\n#计算一下初始值的损失\nprint ('当前损失：',cost_function(X_train, y_train, weight))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 定义线性回归模型\ndef linear_regression(X, y, weight, alpha, iterations): \n    loss_history, weight_history = gradient_descent(X, y, \n                                                    weight, \n                                                    alpha, \n                                                    iterations)\n    print(\"训练最终损失:\", loss_history[-1]) # 打印最终损失\n    y_pred = X.dot(weight_history[-1]) # 预测\n    traning_acc = 100 - np.mean(np.abs(y_pred - y)/y)*100 # 计算准确率\n    print(\"线性回归训练准确率: {:.2f}%\".format(traning_acc))  # 打印准确率\n    return loss_history, weight_history # 返回训练历史记录","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_history, weight_history = linear_regression(X_train, y_train, weight, alpha, iterations) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_history, weight_history = gradient_descent(X_train, y_train, weight, alpha, iterations) \nprint(\"权重历史记录：\", weight_history)\nprint(\"损失历史记录：\", loss_history)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}