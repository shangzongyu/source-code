{"cells":[{"metadata":{},"cell_type":"markdown","source":"实例代码来源于Keras官网\n\nVAE具有模块化设计。编码器，解码器和VAE是3种共享权重的模型。训练VAE模型后，编码器可用于生成潜矢量。通过从均值= 0和std = 1.E的高斯分布中采样潜矢量，可以将解码器用于生成MNIST数字。E具有模块化设计。编码器，解码器和VAE是3种共享权重的模型。训练VAE模型后，编码器可用于生成潜矢量。通过从均值= 0和std = 1的高斯分布中采样潜矢量，可以将解码器用于生成MNIST数字。"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom keras.layers import Lambda, Input, Dense\nfrom keras.models import Model\nfrom keras.datasets import mnist\nfrom keras.losses import mse, binary_crossentropy\nfrom keras.utils import plot_model\nfrom keras import backend as K\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\nimport os\n\n\n# reparameterization trick\n# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n# z = z_mean + sqrt(var) * epsilon\ndef sampling(args):\n    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n\n    # Arguments\n        args (tensor): mean and log of variance of Q(z|X)\n\n    # Returns\n        z (tensor): sampled latent vector\n    \"\"\"\n\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    # by default, random_normal has mean = 0 and std = 1.0\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\n\ndef plot_results(models,\n                 data,\n                 batch_size=128,\n                 model_name=\"vae_mnist\"):\n    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n\n    # Arguments\n        models (tuple): encoder and decoder models\n        data (tuple): test data and label\n        batch_size (int): prediction batch size\n        model_name (string): which model is using this function\n    \"\"\"\n\n    encoder, decoder = models\n    x_test, y_test = data\n    os.makedirs(model_name, exist_ok=True)\n\n    filename = os.path.join(model_name, \"vae_mean.png\")\n    # display a 2D plot of the digit classes in the latent space\n    z_mean, _, _ = encoder.predict(x_test,\n                                   batch_size=batch_size)\n    plt.figure(figsize=(12, 10))\n    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.savefig(filename)\n    plt.show()\n\n    filename = os.path.join(model_name, \"digits_over_latent.png\")\n    # display a 30x30 2D manifold of digits\n    n = 30\n    digit_size = 28\n    figure = np.zeros((digit_size * n, digit_size * n))\n    # linearly spaced coordinates corresponding to the 2D plot\n    # of digit classes in the latent space\n    grid_x = np.linspace(-4, 4, n)\n    grid_y = np.linspace(-4, 4, n)[::-1]\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = np.array([[xi, yi]])\n            x_decoded = decoder.predict(z_sample)\n            digit = x_decoded[0].reshape(digit_size, digit_size)\n            figure[i * digit_size: (i + 1) * digit_size,\n                   j * digit_size: (j + 1) * digit_size] = digit\n\n    plt.figure(figsize=(10, 10))\n    start_range = digit_size // 2\n    end_range = (n - 1) * digit_size + start_range + 1\n    pixel_range = np.arange(start_range, end_range, digit_size)\n    sample_range_x = np.round(grid_x, 1)\n    sample_range_y = np.round(grid_y, 1)\n    plt.xticks(pixel_range, sample_range_x)\n    plt.yticks(pixel_range, sample_range_y)\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.imshow(figure, cmap='Greys_r')\n    plt.savefig(filename)\n    plt.show()\n\n\n# MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimage_size = x_train.shape[1]\noriginal_dim = image_size * image_size\nx_train = np.reshape(x_train, [-1, original_dim])\nx_test = np.reshape(x_test, [-1, original_dim])\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# network parameters\ninput_shape = (original_dim, )\nintermediate_dim = 512\nbatch_size = 128\nlatent_dim = 2\nepochs = 50\n\n# VAE model = encoder + decoder\n# build encoder model\ninputs = Input(shape=input_shape, name='encoder_input')\nx = Dense(intermediate_dim, activation='relu')(inputs)\nz_mean = Dense(latent_dim, name='z_mean')(x)\nz_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n# use reparameterization trick to push the sampling out as input\n# note that \"output_shape\" isn't necessary with the TensorFlow backend\nz = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n# instantiate encoder model\nencoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\nencoder.summary()\nplot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n\n# build decoder model\nlatent_inputs = Input(shape=(latent_dim,), name='z_sampling')\nx = Dense(intermediate_dim, activation='relu')(latent_inputs)\noutputs = Dense(original_dim, activation='sigmoid')(x)\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()\nplot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n\n# instantiate VAE model\noutputs = decoder(encoder(inputs)[2])\nvae = Model(inputs, outputs, name='vae_mlp')\n\nif __name__ == '__main__':\n#     parser = argparse.ArgumentParser()\n#     help_ = \"Load h5 model trained weights\"\n#     parser.add_argument(\"-w\", \"--weights\", help=help_)\n#     help_ = \"Use mse loss instead of binary cross entropy (default)\"\n#     parser.add_argument(\"-m\",\n#                         \"--mse\",\n#                         help=help_, action='store_true')\n#     args = parser.parse_args()\n    models = (encoder, decoder)\n    data = (x_test, y_test)\n\n    # VAE loss = mse_loss or xent_loss + kl_loss\n#     if args.mse:\n#         reconstruction_loss = mse(inputs, outputs)\n#     else:\n    reconstruction_loss = binary_crossentropy(inputs,outputs)\n\n    reconstruction_loss *= original_dim\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n    vae.summary()\n    plot_model(vae,\n               to_file='vae_mlp.png',\n               show_shapes=True)\n\n#     if args.weights:\n#         vae.load_weights(args.weights)\n#     else:\n        # train the autoencoder\n    vae.fit(x_train,\n            epochs=epochs,\n            batch_size=batch_size,\n            validation_data=(x_test, None))\n    vae.save_weights('vae_mlp_mnist.h5')\n\n    plot_results(models,\n                 data,\n                 batch_size=batch_size,\n                 model_name=\"vae_mlp\")","execution_count":4,"outputs":[{"output_type":"stream","text":"Model: \"encoder\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nencoder_input (InputLayer)      (None, 784)          0                                            \n__________________________________________________________________________________________________\ndense_10 (Dense)                (None, 512)          401920      encoder_input[0][0]              \n__________________________________________________________________________________________________\nz_mean (Dense)                  (None, 2)            1026        dense_10[0][0]                   \n__________________________________________________________________________________________________\nz_log_var (Dense)               (None, 2)            1026        dense_10[0][0]                   \n__________________________________________________________________________________________________\nz (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n                                                                 z_log_var[0][0]                  \n==================================================================================================\nTotal params: 403,972\nTrainable params: 403,972\nNon-trainable params: 0\n__________________________________________________________________________________________________\nModel: \"decoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nz_sampling (InputLayer)      (None, 2)                 0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 512)               1536      \n_________________________________________________________________\ndense_12 (Dense)             (None, 784)               402192    \n=================================================================\nTotal params: 403,728\nTrainable params: 403,728\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"vae_mlp\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nencoder_input (InputLayer)   (None, 784)               0         \n_________________________________________________________________\nencoder (Model)              [(None, 2), (None, 2), (N 403972    \n_________________________________________________________________\ndecoder (Model)              (None, 784)               403728    \n=================================================================\nTotal params: 807,700\nTrainable params: 807,700\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n  'be expecting any data to be passed to {0}.'.format(name))\n","name":"stderr"},{"output_type":"error","ename":"NameError","evalue":"name 'args' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-c829430110da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    176\u001b[0m                show_shapes=True)\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"]}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}